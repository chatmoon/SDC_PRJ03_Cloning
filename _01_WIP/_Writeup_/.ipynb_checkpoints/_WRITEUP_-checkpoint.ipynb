{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Writeup | Behavioral Cloning**   \n",
    "5 min read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract —** this notebook is the writeup of the Behavioral Cloning project as part of the SELF-DRIVING CAR nanodegree program. We apply Deep Learning to clone the behavior of a human driver by getting training data from examples of human driving in a simulator. Then these data feed into a convolutional neural network (CNN) to map features from camera images directly to steering commands. This way the CNN learns to predict the appropriate steering angle when the car drives in autonomous mode in the simulator.   \n",
    "\n",
    "The goals of this project are broken down into the following steps:   \n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "**Note:** the architecture has been inspired by the [nvidia neural network](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) before being tweaked a bit using the [Keras](https://keras.io/) deep learning library with Tensorflow backend. The convolutional neural network has been trained only with the sample data provided by Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Rubric Points   \n",
    "*Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.*   \n",
    "\n",
    "### 1. Files Submitted & Code Quality\n",
    "\n",
    "#### 1.1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "\n",
    "My project includes the following files:\n",
    "* model.py containing the script to create and train the model\n",
    "* tools.py containing the generator to pull pieces of the data and process them on the fly \n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup_report.md or writeup_report.pdf summarizing the results\n",
    "\n",
    "#### 1.2. Submission includes functional code\n",
    "Using the Udacity provided simulator and a modified version of the drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.3. Submission code is usable and readable\n",
    "\n",
    "The model.py and tools.py files contains the code for training and saving the convolution neural network. The files show the pipeline I used for training and validating the model, and they contain comments to explain how the code works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Architecture   \n",
    "\n",
    "#### 2.1. An appropriate model architecture has been employed\n",
    "\n",
    "There are the following layers : a normalization layer, 3 convolutional2D layers, 1 flatten layer and 4 fully-connected layers. ELU ([Exponential linear unit](https://www.quora.com/How-does-ELU-activation-function-help-convergence-and-whats-its-advantages-over-ReLU-or-sigmoid-or-tanh-function)) activation functions are added at each convolutional2D and fully-connected layers. The dropout regularization technique is inserted between the last convolutional2D layer and the flatten layer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [final model](https://github.com/davisjazz/SDC_PRJ03_Cloning/blob/develop/_00_BSF/_Coding_/_1_pre-train/BSF_model_171203-2024.py) consisted of the following layers:   \n",
    "\n",
    "| Layer         \t\t  |     Description\t        \t\t\t\t         |   \n",
    "|:-----------------------:|:------------------------------------------------:|    \n",
    "| input         \t\t  | 32 x 155 x 3 \t\t\t\t\t\t\t         |      \n",
    "| normalization           | lambda x: x/255.0-0.5                            |   \n",
    "| convolution2D        \t  | { filter: 16 ; stride: 2x2 ; kernel size : 5x5 } |   \n",
    "| activation function\t  | ELU (Exponential linear unit)                    |   \n",
    "| convolution2D        \t  | { filter: 32 ; stride: 2x2 ; kernel size : 5x5 } |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| convolution2D        \t  | { filter: 64 ; stride: 2x2 ; kernel size : 5x5 } |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| regularization \t\t  | dropout(keep_prob = 0.5)  \t\t\t             |   \n",
    "| flatten\t      \t\t  |    \t\t\t    \t\t\t        \t         |   \n",
    "| fully connected (Dense) | { 100 hidden units }   \t\t\t                 |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| fully connected (Dense) | { 50 hidden units }   \t\t\t                 |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| fully connected (Dense) | { 10 hidden units }   \t\t\t                 |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| fully connected (Dense) | { 1 hidden unit }   \t\t\t                 |   \n",
    "\n",
    "\n",
    "**Note:** I do not use a max pooling layer. For more details, read the [Geoffrey Hinton's comments on max pooling](https://mirror2image.wordpress.com/2014/11/11/geoffrey-hinton-on-max-pooling-reddit-ama/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Attempts to reduce overfitting in the model\n",
    "\n",
    "The model contains one dropout layer in order to reduce overfitting.\n",
    "\n",
    "The model was trained and validated on different data sets to ensure that the model was not overfitting (code line 44-50). The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Model parameter tuning\n",
    "\n",
    "The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 70) except for the transfer learning phase where I changed the default learning rate from 1e-3 to 8.5e-4.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.4. Appropriate training data\n",
    "\n",
    "I tried to collect data using the keyboard arrows but it was very challenging and at the end the data were not as good as expected to train properly the neural network. Then the sample data provided by Udacity have been used to train the model. Jitteration and preprocessing techniques have shown to be very effective ways to improve the neural network performances as I have learnt in the Traffic Sign Recognition project. In order to keep the vehicle driving on the road the data have been also preprocessed and jittered before fine tunning the neural network. \n",
    "\n",
    "For details about how I created the training data, see the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Training Strategy   \n",
    "\n",
    "#### 3.1. Solution Design Approach\n",
    "\n",
    "In summary,\tit was an iterative approach with much trial and error. Initially, I have trained a convolution neural network (CNN) model similar to the [Nvidia CNN](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) with the original sample data set. Next, I modified the CNN model and tweaked a few parameters. Then I preprocessed and artificially augmented data to fine tune the model using the transfer learning technique.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1. The first attempt: Nvidia convolution neural network\n",
    "Initially I used a convolution neural network model similar to the [Nvidia CNN](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) as discribed below:\n",
    "> *fig.1: first adaptation of the Nvidia CNN*\n",
    "![fig.1](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_model_SPIN.PNG)   \n",
    "\n",
    "The Nvidia model might be appropriate because it learns to drive in track with or without lane markings or in areas with unclear visual guidance such as on unpaved roads. It automatically learns to detecte useful road features with only the steering angle as the training signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the vehicle could stay on the track one when the model was tested by running it through the simulator. **However, the car tends to drive on the left lane line as it is shown in the video.1.**   \n",
    "\n",
    "> *video.1 - result 1*   \n",
    "\n",
    ">[![video.1](https://img.youtube.com/vi/2C4R3JE7Czc/0.jpg)](https://youtu.be/uxNCMQ3gk-Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 3.1.2. Overfitting and countermeasure   \n",
    "In order to gauge how well the model was working, I split the image and steering angle data into a training and validation set. This first model had a low mean squared error on the training set but a high mean squared error on the validation set. This implied that the model was overfitting. \n",
    "\n",
    "To combat the overfitting, I used the following techniques:\n",
    "- train the model with more data (augmentation and jitteration)\n",
    "- stop the training when the loss has stopped improving\n",
    "- add a dropout layer as a regularization technique\n",
    "\n",
    "I also removed all images and the related steering angles driving the car off the track from the sample dataset.\n",
    "\n",
    "If I would have more time, I would implement [the cross-validation technique](https://github.com/keras-team/keras/issues/1711) as well. But the vehicle could stay on the track one when the model was tested by running it through the simulator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3. Fine tuning   \n",
    "The video.1 shows the vehicle drove on the lane line of the track one. To improve the driving behavior, I first modified the convolutional neural network by removing a few convolutional2D layers and then I tweaked a few parameters like the initial learning rate in the transfer learning phase, the number of convolution layers and their related parameters, the regulation functions, to name but a few.   \n",
    "\n",
    "From that point, I have fine tuned the model twice using the transfer learning technique: first with more training data, and then with a different compensation rate of the steering angle. For details about how I created the training data, see the next section.   \n",
    "\n",
    "At first sight, the video.2 shows the fine tuning with more training data improved dramatically the car behavior by driving much less on the lane lines, more in the center of the track from the beginning, then over the brige, passing successfully the first turn with its dirty border but **it drove off the track in the second turn**:  \n",
    "> *video.2 - result 2*   \n",
    "\n",
    ">[![video.2](https://img.youtube.com/vi/_FEcHhhy9lc/0.jpg)](https://youtu.be/_FEcHhhy9lc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to overcome the case of the vehicle leaving the road, I have fine tuned a second time the model by increasing the compensation rate of the steering angle. Finally, the video.3 shows the vehicle can drive autonomously around the track one without leaving the road:       \n",
    "\n",
    "> *video.3 - result 3*   \n",
    "\n",
    ">[![video.3](https://img.youtube.com/vi/McrmB3ZIA18/0.jpg)](https://youtu.be/McrmB3ZIA18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2. Creation of the Training Set & Training Process\n",
    "\n",
    "There are a few challenges. The car has to drive in the center of the track from the beginning, then over the brige, passing successfully a few sharp turns and some with dirty border instead of clean lane lines, not to mention a few cast shadows on the track.\n",
    "\n",
    "Despite this, I chose to train the model with the sample data provided by Udacity without collecting additional data.\n",
    "\n",
    "##### 3.2.1. Data exploration\n",
    "\n",
    "Initially, the sample data is comprised of 8036 units. Each unit is made of a steering angle value and the associated images from the center, left and right cameras. Here is a video based on all images from the center camera found in the sample data set cleaned of a few units that drove the car off the track: [video.4](https://youtu.be/YzftbxF7-_w)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The throttle, the brake and the speed values were set aside in this project. I will use them at a later date when I have more time.   \n",
    "\n",
    "\n",
    "\n",
    "To begin with, the figure 2 reveals the data is imbalanced:   \n",
    "\n",
    "> *fig.2 - number of occurence per steering angle value*\n",
    "![fig.2](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_dataChart_z-aug0.png)\n",
    "\n",
    "As we can observe, there is a concentration of steering angles of 0 radian. The track one is straight driving and most of the time with light steering and a few sharpe turns. In this case, the value of mean_squared_error metric might be excellent on paper but it is only reflecting the underlying steering angle distribution. Consequently, the model will have the tendency to decide that the best thing to do is to often predict a steering angle of 0 degree. In other words, the sample data has too much straight driving data with few turning data. It explains why the car went off the track at the first turns when the model is trained with the original sample data *(see video.1)*.\n",
    "\n",
    "In order to remedy this problem, I have learnt in the previous project that the best way is to rebalance the data. For those purposes, there are two options:   \n",
    "- either increase right/left turning data\n",
    "- or decrease the straight driving data   \n",
    "\n",
    "I chose to increase right/left turning data to both rebalance the data set and to reduce overfitting in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2. Rebalance the data set - increasing right/left turning data  \n",
    "\n",
    "For a start, I have at my disposal images from the left and rigth cameras that I can use to create some artificial 'recovery' data, i.e. data that enable the model to learn how to recover when the vehicle drives away from the center of the track. For example, if we look at the images from the left camera in the figure 3, the vehicle seems to be closer to the left lane than it actually is.\n",
    "\n",
    "> *fig.3 - example of images from the center, the left and rigth cameras*\n",
    "![fig.3](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_unit_2016_12_01_13_33_56_606.jpg)\n",
    "\n",
    "If it was an image from the center camera, we would like to teach the model to steer the vehicle back on the center of the track. To increase the left turning data, I used the left image as an input of the model and I added a readjustment angle to the current steering angle value as the desired output value. I repeated it with the right images but this time subtracting the readjustment angle to the steering angle.   \n",
    "\n",
    "The figure 4 shows the steering angle distribution after using the left and right images:   \n",
    "> *fig.4 - rebalance - part1 / number of occurence per steering angle value*\n",
    "![fig.4](\n",
    "https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_dataChart_zy-aug01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, I randomly flipped images and angles in order to augment the data set and rebalance right and left images. At the end of this increase of data, I had **68,430 number of data points**. The figure 5 shows the new steering angle distribution:   \n",
    "\n",
    "> *fig.5 - rebalance - part2 / number of occurence per steering angle value*\n",
    "![fig.5](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_dataChart_zyx-aug012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3. Preprocess the images - cropping, resizing, changing brightness\n",
    "Next, I preprocessed this data by cropping and resizing images.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *fig.6 - cropped and resized images*\n",
    "![fig.6](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_PPro_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I randomly shuffled the data set and put 20% of the data into a validation set. And I used this augmented data for training the model. The training is interrupted when the validation loss isn’t decreasing anymore by using an EarlyStopping callback. And if by chance it does not interrupt itself, the maximun number of epochs has been set between 5 and 10. I also used an adam optimizer so that manually training the learning rate wasn't necessary. As a result, the car drove off the track in the second turn after passing the first bend, the one with the dirty border *(see video.2)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to address this shortcoming, I fine tuned the model using a greater reajustment angle value. I have changed the 0.20 radian by 0.25 radian of reajustment. At the end, I had **128,604 number of data points in total** *(see figure 6)*. And it solved the problem. The vehicle could finally drive the whole track one *(see video.3)*.\n",
    "> *fig.6 - fine tune - number of occurence per steering angle value*\n",
    "![fig.6](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_image_/_dataChart_wv-aug12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What's next\n",
    "#### 4.1. Track two\n",
    "\n",
    "The model does not generalize well. As the video.4 shows the vehicle drove off having a tendency to get around the cast shadows on the track.\n",
    "\n",
    "> *video.4 - result 4*  \n",
    "\n",
    ">[![video.4](https://img.youtube.com/vi/4f8VXsyo5E0/0.jpg)](https://youtu.be/4f8VXsyo5E0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOUS ETES ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RàF\n",
    "<< 1 example of the brithness modif + comment about the reason and the method >>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNEXE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RÀF:\n",
    "- commentaire sur resultats track one et two: problème avec les hombres portées\n",
    "- réécrire le code à la facon de Siraj et Udacity (apprendre architecture code)\n",
    "- si du temps, intégré capsule network pour voir\n",
    "- puis passé au plus vite aux autres projets au plus vite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3. Preprocess the images - cropping, resizing, changing brightness\n",
    "Next, I preprocessed this data by cropping and resizing images, and also changing brightness randomly to simulate day and night conditions.   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
