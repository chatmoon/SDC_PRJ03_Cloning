{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Writeup | Behavioral Cloning**   \n",
    "5 min read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract â€”** this notebook is the writeup of the Behavioral Cloning project as part of the SELF-DRIVING CAR nanodegree program. We apply Deep Learning to clone the behavior of a human driver by getting training data from examples of human driving in a simulator. Then these data feed into a convolutional neural network (CNN) to map pixels from camera images directly to steering commands. This way the CNN learns to predict the appropriate steering angle when the car drives in autonomous mode in the simulator.   \n",
    "\n",
    "The goals of this project are broken down into the following steps:   \n",
    "```\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "```\n",
    "\n",
    "**Note:** the architecture has been inspired by the [nvidia neural network](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) before being tweaked a bit using the [Keras](https://keras.io/) deep learning library with Tensorflow backend. The convolutional neural network has been trained only with the sample data provided by Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[//]: # (Image References)\n",
    "[image1]: ./examples/placeholder.png \"Model Visualization\"\n",
    "[image2]: ./examples/placeholder.png \"Grayscaling\"\n",
    "[image3]: ./examples/placeholder_small.png \"Recovery Image\"\n",
    "[image4]: ./examples/placeholder_small.png \"Recovery Image\"\n",
    "[image5]: ./examples/placeholder_small.png \"Recovery Image\"\n",
    "[image6]: ./examples/placeholder_small.png \"Normal Image\"\n",
    "[image7]: ./examples/placeholder_small.png \"Flipped Image\"   \n",
    "\n",
    "---\n",
    "## Rubric Points   \n",
    "*Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.*   \n",
    "\n",
    "### Files Submitted & Code Quality\n",
    "\n",
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "\n",
    "My project includes the following files:\n",
    "```\n",
    "* model.py containing the script to create and train the model\n",
    "* tools.py containing the generator to pull pieces of the data and process them on the fly \n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup_report.md or writeup_report.pdf summarizing the results\n",
    "```   \n",
    "\n",
    "#### 2. Submission includes functional code\n",
    "Using the Udacity provided simulator and a modified version of the drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. Submission code is usable and readable\n",
    "\n",
    "The model.py and tools.py files contains the code for training and saving the convolution neural network. The files show the pipeline I used for training and validating the model, and they contain comments to explain how the code works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. An appropriate model architecture has been employed\n",
    "\n",
    "There are the following layers : a normalization layer, 3 convolutional2D layers, 1 flatten layer and 4 fully-connected layers. ELU ([Exponential linear unit](https://www.quora.com/How-does-ELU-activation-function-help-convergence-and-whats-its-advantages-over-ReLU-or-sigmoid-or-tanh-function)) activation functions are added at each convolutional2D and fully-connected layers. The dropout regularization technique is inserted between the last convolutional2D layer and the flatten layer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [final model](https://github.com/davisjazz/SDC_PRJ03_Cloning/blob/develop/_00_BSF/_Coding_/_1_pre-train/BSF_model_171203-2024.py) consisted of the following layers:   \n",
    "\n",
    "| Layer         \t\t  |     Description\t        \t\t\t\t         |   \n",
    "|:-----------------------:|:------------------------------------------------:|    \n",
    "| input         \t\t  | 32 x 155 x 3 \t\t\t\t\t\t\t         |      \n",
    "| normalization           | lambda x: x/255.0-0.5                            |   \n",
    "| convolution2D        \t  | { filter: 16 ; stride: 2x2 ; kernel size : 5x5 } |   \n",
    "| activation function\t  | Exponential linear unit (ELU)                    |   \n",
    "| convolution2D        \t  | { filter: 32 ; stride: 2x2 ; kernel size : 5x5 } |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| convolution2D        \t  | { filter: 64 ; stride: 2x2 ; kernel size : 5x5 } |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| regularization \t\t  | dropout(keep_prob = 0.5)  \t\t\t             |   \n",
    "| flatten\t      \t\t  |    \t\t\t    \t\t\t        \t         |   \n",
    "| fully connected (Dense) | Dense { 100 hidden units }   \t\t\t         |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| fully connected (Dense) | Dense { 50 hidden units }   \t\t\t         |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| fully connected (Dense) | Dense { 10 hidden units }   \t\t\t         |   \n",
    "| activation function\t  | ELU                     \t\t\t             |   \n",
    "| fully connected (Dense) | Dense { 1 hidden unit }   \t\t\t             |   \n",
    "\n",
    "\n",
    "**Note:** I do not use a max pooling layer. For more details, read the [Geoffrey Hinton's comments on max pooling](https://mirror2image.wordpress.com/2014/11/11/geoffrey-hinton-on-max-pooling-reddit-ama/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Attempts to reduce overfitting in the model\n",
    "\n",
    "The model contains one dropout layer in order to reduce overfitting.\n",
    "\n",
    "The model was trained and validated on different data sets to ensure that the model was not overfitting (code line 44-50). The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model parameter tuning\n",
    "\n",
    "The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 70) except for the transfer learning phase where I changed the default learning rate from 1e-3 to 8.5e-4.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4. Appropriate training data\n",
    "\n",
    "I tried to collect data using the keyboard arrows but it was very challenging and at the end the data were not as good as expected to train properly the neural network. Then the sample data provided by Udacity have been used to train the model. In order to keep the vehicle driving on the road they have been also preprocessed and jittered before fine tunning the neural network. Jitteration and preprocessing techniques have shown to be very effective ways to improve the neural network performances as I have learnt in the Traffic Sign Recognition project.  \n",
    "\n",
    "For details about how I created the training data, see the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. Solution Design Approach\n",
    "\n",
    "It was an iterative approach with much trial and error.\n",
    "\n",
    "Initially I used a convolution neural network model similar to the [Nvidia CNN](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) as discribed below:\n",
    "![fig.2](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_model_SPIN.PNG)\n",
    "\n",
    "\n",
    "The Nvidia model might be appropriate because it learns to drive in track with or without lane markings or in areas with unclear visual guidance such as on unpaved roads. It automatically learns to detecte useful road features with only the steering angle as the training signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. I found that my first model had a low mean squared error on the training set but a high mean squared error on the validation set. This implied that the model was overfitting. \n",
    "\n",
    "To combat the overfitting, I used the following techniques:\n",
    "- I trained the model with more data (augmentation and jitteration)\n",
    "- I removed all images and the related steering angles driving the car off the track from the sample dataset\n",
    "- I stopped the training when the loss has stopped improving\n",
    "- I added a dropout layer as a regularization technique\n",
    "\n",
    "If I would have more time, I would implement [the cross-validation technique](https://github.com/keras-team/keras/issues/1711) as well. But [the vehicle could stay on the track one](https://youtu.be/uxNCMQ3gk-Q) when the model was tested by running it through the simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a few spots where the vehicle drove on the lane line of the track. To improve the driving behavior in these cases, I first tried to modify the convolutional neural network by removing a few convolutional2D and then I tweaked it.   \n",
    "\n",
    "At first sight, [the car behavior was dramatically improved](https://youtu.be/_FEcHhhy9lc) by driving perfectly well in the center of the track from the beginning, then over the brige, passing successfully the first turn with its dirty border but it drove off the track in the second turn.   \n",
    "\n",
    "From that point, I have fine tuned the model with more jittered data and I modified the compensation rate of the steering angle.\n",
    "\n",
    "At the end of the process, [the vehicle is able to drive autonomously around the track without leaving the road](https://youtu.be/McrmB3ZIA18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VOS ETES ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. Creation of the Training Set & Training Process\n",
    "\n",
    "To capture good driving behavior, I first recorded two laps on track one using center lane driving. Here is an example image of center lane driving:\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to .... These images show what a recovery looks like starting from ... :\n",
    "\n",
    "![alt text][image3]\n",
    "![alt text][image4]\n",
    "![alt text][image5]\n",
    "\n",
    "Then I repeated this process on track two in order to get more data points.\n",
    "\n",
    "To augment the data sat, I also flipped images and angles thinking that this would ... For example, here is an image that has then been flipped:\n",
    "\n",
    "![alt text][image6]\n",
    "![alt text][image7]\n",
    "\n",
    "Etc ....\n",
    "\n",
    "After the collection process, I had X number of data points. I then preprocessed this data by ...\n",
    "\n",
    "\n",
    "I finally randomly shuffled the data set and put Y% of the data into a validation set. \n",
    "\n",
    "I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was Z as evidenced by ... I used an adam optimizer so that manually training the learning rate wasn't necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VOS ETES ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Creation of the Training Set & Training Process\n",
    "\n",
    "To capture good driving behavior, I first recorded two laps on track one using center lane driving. Here is an example image of center lane driving:\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to .... These images show what a recovery looks like starting from ... :\n",
    "\n",
    "![alt text][image3]\n",
    "![alt text][image4]\n",
    "![alt text][image5]\n",
    "\n",
    "Then I repeated this process on track two in order to get more data points.\n",
    "\n",
    "To augment the data sat, I also flipped images and angles thinking that this would ... For example, here is an image that has then been flipped:\n",
    "\n",
    "![alt text][image6]\n",
    "![alt text][image7]\n",
    "\n",
    "Etc ....\n",
    "\n",
    "After the collection process, I had X number of data points. I then preprocessed this data by ...\n",
    "\n",
    "\n",
    "I finally randomly shuffled the data set and put Y% of the data into a validation set. \n",
    "\n",
    "I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was Z as evidenced by ... I used an adam optimizer so that manually training the learning rate wasn't necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ANNEXE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My model is an adaptation of the [nvidia architecture](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf). \n",
    ">![fig.1](https://raw.githubusercontent.com/davisjazz/SDC_PRJ03_Cloning/develop/_01_WIP/_Writeup_/_model_1%20SPIN_2%20BSF.png)\n",
    "\n",
    "My model consists of *a convolution neural network with 3x3 filter sizes and depths between 32 and 128 (model.py lines 18-24)*. It is an adaptation of the [nvidia architecture](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)\n",
    "\n",
    "*The model includes ELU layers to introduce nonlinearity (code line 20), and the data is normalized in the model using a Keras lambda layer (code line 18)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
